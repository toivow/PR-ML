{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73ee399",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc1d17",
   "metadata": {},
   "source": [
    "## Discrete control problems - Q-learning\n",
    "Q-learning is the fundamental algorithm of RL that maintains so called Q-table $Q(s,a)$ of total reward when in the state $s$ and action $a$ is taken. Q-table is updated using either random exploration (random actions) or $\\epsilon$-greedy algorithm. The update rule is\n",
    "$$\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+\\alpha\\left( R_{t+1}+\\gamma \\max_a Q(S_{t+1},a)-Q(S_t,A_t)\\right) ,\n",
    "$$\n",
    "where $\\alpha \\in ]0,1]$ is the learning rate (how much new observations are used) and $\\gamma \\in [0,1[$ is reward delay. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec1b8c",
   "metadata": {},
   "source": [
    "### Demo 1: Frozen lake\n",
    "\n",
    "Frozen lake has many uncertainties. For example, when you try to move (left, right, up or down) you sometimes do not move, you sometimes move to wrong direction. Moreover, in some locations the ice is too thin and you fall to cold water (game over). Can you learn to find your way over the frozen lake? You need to particularly understand what happens with learning when you move from non-slippery ice to slippery ice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4641b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c55948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "print(\"Action size: \", action_size)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "print(\"State size: \", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf654a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    #action = np.random.randint(0,4) # 0:Left 1:Down 2: Right, 3: Up\n",
    "    action = int(input('0/left 1/down 2/right 3/up:'))\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    time.sleep(1.0) \n",
    "    print(f'S_t+1={new_state}, R_t+1={reward}, done={done}')\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b5e9f",
   "metadata": {},
   "source": [
    "Let's initialize Q-table structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c583dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable_history = []\n",
    "score_history = []\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "total_episodes = 10000        # Total episodes\n",
    "learning_rate = 1.0          # Learning rate alpha\n",
    "max_steps = 100              # Max steps per episode\n",
    "gamma = 0.9                  # Discounting rate\n",
    "\n",
    "# Exploration parameters (not really needed)\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.001            # Minimum exploration probability \n",
    "decay_rate = 0.00005             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb880aca",
   "metadata": {},
   "source": [
    "Let's define an evaluation function that runs the current policy (action that maximises Q-value in each state) and return average reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(qtable_, num_of_episodes_, max_steps_):\n",
    "    env.reset()\n",
    "    total_test_episodes = 1000\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_of_episodes_):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for step in range(max_steps_):\n",
    "            action = np.argmax(qtable_[state,:])\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "        \n",
    "            if done:\n",
    "                rewards.append(total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards)/num_of_episodes_\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe29a1",
   "metadata": {},
   "source": [
    "Let's test the initial Q-table full of zeros. Note that you need to define number of episodes and max number of steps so that vaues stabilize over random runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_policy(qtable,1000,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1dd696",
   "metadata": {},
   "source": [
    "Let's make optimal Q-table manually and test average reward it achieves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable_opt = np.zeros((state_size, action_size))\n",
    "qtable_opt[0,:] = [0,0,1,0] # Row 1\n",
    "qtable_opt[1,:] = [0,0,1,0] # \n",
    "qtable_opt[2,:] = [0,1,0,0] # \n",
    "qtable_opt[3,:] = [1,0,0,0] # \n",
    "qtable_opt[4,:] = [0,1,0,0] # Row 2\n",
    "qtable_opt[5,:] = [0,1,0,0] # \n",
    "qtable_opt[6,:] = [0,1,0,0] # \n",
    "qtable_opt[7,:] = [1,0,0,0] # \n",
    "qtable_opt[8,:] = [0,0,1,0] # Row 3\n",
    "qtable_opt[9,:] = [0,0,1,0] # \n",
    "qtable_opt[10,:] = [0,1,0,0] # \n",
    "qtable_opt[11,:] = [0,1,0,0] # \n",
    "qtable_opt[12,:] = [0,0,1,0] # Row 3\n",
    "qtable_opt[13,:] = [0,0,1,0] # \n",
    "qtable_opt[14,:] = [0,0,1,0] # \n",
    "qtable_opt[15,:] = [0,0,0,0] # \n",
    "print(eval_policy(qtable_opt,1000,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca2e74",
   "metadata": {},
   "source": [
    "OK. let's try to learn the optimal Q-table using the Q-learning update rule. Particularly we should study optimal actions near the goal to make sure the system converged to correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fec227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "episode_count = 0\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = np.random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration random integer in [0,3]\n",
    "        else:\n",
    "            #action = env.action_space.sample() # OpenAI Gym provides this\n",
    "            action = np.random.randint(0,4)\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    #epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "    episode_count = episode + 1\n",
    "    if episode_count % 1000 == 0 or episode_count == 1:\n",
    "        print(eval_policy(qtable,1000,100))\n",
    "        #qtable_history.append(qtable)\n",
    "        #score_history.append(sum(rewards)/episode_count)\n",
    "        #save_canvas(qtable, 800, 800, filename = \"./output/FrozenLake_ep\" + str(episode_count) + \".png\")\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aea38d",
   "metadata": {},
   "source": [
    "## Continuous control problems\n",
    "Discrete problems mainly appear in games and other virtual environments, but robot measurements are continuous (continuous states) and often also control signals are continuous. It is important to understand what works there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab3503",
   "metadata": {},
   "source": [
    "### Demo 2: CartPole controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89aaafe",
   "metadata": {},
   "source": [
    "Let's load the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14068790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "import gym # pull the environment\n",
    "import time # to get the time\n",
    "import math # needed for calculations\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79341f14",
   "metadata": {},
   "source": [
    "CartPole provides observations of the cart location in $[-4.8,+4.8]$ units, cart acceleration in $[-\\infty,+\\infty]$, pole angle in radians in $[-24^\\circ,+24^\\circ]$ and pole acceleration in in $[-\\infty,+\\infty]$. The control signal is a discrete \"push\" of the cart to the left, $a=0$, or right, $a=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abf742",
   "metadata": {},
   "source": [
    "Let's have an example run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f813d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.random.randint(0, 2) # Random action\n",
    "    #action = 1 # Fixed action, 0 or 1\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    print(new_state)\n",
    "    print(reward)\n",
    "    print(done)\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52854eae",
   "metadata": {},
   "source": [
    "Q-learning needs discrete values so let's discretize the space observation space. Number of bins is important variable here. For example, 20 bins may converge well while 10 almost never."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_window_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "num_of_bins = 40\n",
    "cart_loc_bins = np.linspace(env.observation_space.low[0],env.observation_space.high[0],num_of_bins)\n",
    "cart_acc_bins = np.linspace(-10,+10,num_of_bins)\n",
    "pole_angle_bins = np.linspace(env.observation_space.low[2],env.observation_space.high[2],num_of_bins)\n",
    "pole_acc_bins = np.linspace(-10,+10,num_of_bins)\n",
    "def get_discrete_state(state):\n",
    "    cart_loc = np.argmin(np.abs(cart_loc_bins-state[0]))\n",
    "    cart_acc = np.argmin(np.abs(cart_acc_bins-state[1]))\n",
    "    pole_ang = np.argmin(np.abs(pole_angle_bins-state[2]))\n",
    "    pole_acc = np.argmin(np.abs(pole_acc_bins-state[3]))\n",
    "    return tuple([cart_loc, cart_acc, pole_ang, pole_acc])\n",
    "                         \n",
    "    #discrete_state = state/np_array_window_size+ np.array([15,10,1,10])\n",
    "    #return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685cc433",
   "metadata": {},
   "source": [
    "Let's re-run with discrete states. By running the code with actions 0 and 1 several times, we found that during the valid episode step (done is False) the indeces run from -14..14, -14..14, -19..+19, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.random.randint(0, 2)\n",
    "    #action = 1\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    print(get_discrete_state(new_state))\n",
    "    print(reward)\n",
    "    print(done)\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 40000\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "Observation = [30, 30, 50, 50]\n",
    "\n",
    "\n",
    "epsilon = 1\n",
    "\n",
    "epsilon_decay_value = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "\n",
    "q_table = np.random.uniform(low=0, high=1, size=(num_of_bins, num_of_bins, num_of_bins, num_of_bins, env.action_space.n))\n",
    "\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e467e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(EPISODES + 1): # go through the episodes\n",
    "    t0 = time.time() # set the initial time\n",
    "    discrete_state = get_discrete_state(env.reset()) # get the discrete start for the restarted environment \n",
    "    done = False\n",
    "    episode_reward = 0 # reward starts as 0 for each episode\n",
    "\n",
    "    if episode % 2000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[discrete_state]) # take cordinated action\n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) # do a random ation\n",
    "\n",
    "        #c = env.step(action) # step action to get new states, reward, and the \"done\" status.\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward # add the reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % 2000 == 0: # render\n",
    "            env.render()\n",
    "\n",
    "        if not done: # update q-table\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.05: # epsilon modification\n",
    "        if episode_reward > prior_reward and episode > 10000:\n",
    "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "    \n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    t1 = time.time() # episode has finished\n",
    "    episode_total = t1 - t0 # episode total time\n",
    "    total = total + episode_total\n",
    "\n",
    "    total_reward += episode_reward # episode total reward\n",
    "    prior_reward = episode_reward\n",
    "\n",
    "    if episode % 1000 == 0: # every 1000 episodes print the average time and the average reward\n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf978ab",
   "metadata": {},
   "source": [
    "Let's test how well it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = env.reset()\n",
    "print(new_state)\n",
    "for s in range(0,100):\n",
    "    #action = np.random.randint(0, 2)\n",
    "    discrete_state = get_discrete_state(new_state)\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    print(get_discrete_state(new_state))\n",
    "    print(reward)\n",
    "    print(done)\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ee5ad",
   "metadata": {},
   "source": [
    "It might be useful to think why epsilon sampling (with low value) might still be useful when the controller (policy) is used in real case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c304e",
   "metadata": {},
   "source": [
    "### Demo 3: CartPole using Deep Q-Learning (DQN)\n",
    "\n",
    "In this demo we use the Deep Q-Learning Network proposed by Mnih et al. in 2013. Their idea allowed the fundamental algorithms that helped computers to learn play computer games just by playing them and collecting experiences to train a deep network policy. In their case input is the current image of game that cannot anymore be discretized but needs to be treated as continuous state. In this case $Q(s,a)$ depends on continuous $D$-dimensional state $s \\in \\mathbb{R}^D$ and discrete actions $a = 0, 1, 2, \\ldots$ that can be encoded as one-hot input.\n",
    "\n",
    "See https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b28058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "import gym # pull the environment\n",
    "import time # to get the time\n",
    "import math # needed for calculations\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b14169",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializer = tf.keras.initializers.Zeros()\n",
    "#\n",
    "#model = tf.keras.models.Sequential([\n",
    "#  tf.keras.layers.InputLayer(input_shape=(4,)),\n",
    "#  tf.keras.layers.Dense(32, activation='relu', kernel_initializer=initializer),\n",
    "#  tf.keras.layers.Dense(32, activation='relu', kernel_initializer=initializer),\n",
    "#  tf.keras.layers.Dense(2, activation='linear', kernel_initializer=initializer)\n",
    "#])\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(4,)),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(2, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01392627",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb4159",
   "metadata": {},
   "source": [
    "Let's test using initial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dqn(dqn_, num_of_episodes_, max_steps_):\n",
    "    env.reset()\n",
    "    total_test_episodes = 1000\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_of_episodes_):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for step in range(max_steps_):\n",
    "            action = np.argmax(dqn_.predict(state.reshape(1,-1)))\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "        \n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        rewards.append(total_rewards)\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards)/num_of_episodes_\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37368355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Current DQN perf {eval_dqn(model,10,50)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ad5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = np.array(env.reset())\n",
    "done = False\n",
    "while not done:\n",
    "    print(model.predict(new_state.reshape(1,-1)))\n",
    "    action = np.argmax(model.predict(new_state.reshape(1,-1)))\n",
    "    print(action)\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    print(new_state)\n",
    "    print(reward)\n",
    "    print(done)\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623aa57",
   "metadata": {},
   "source": [
    "DQN main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01628ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_episodes = 10000\n",
    "gamma = 0.9\n",
    "epsilon = 1.0\n",
    "epsilon_decay_value = 0.99995\n",
    "\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "buffer_size = 10000\n",
    "tr_batch_size = 100\n",
    "tr_freq = 10\n",
    "\n",
    "state_buffer = np.zeros((buffer_size,4))\n",
    "reward_buffer = np.zeros((buffer_size,1))\n",
    "action_buffer = np.zeros((buffer_size,1))\n",
    "done_buffer = np.zeros((buffer_size,1))\n",
    "new_state_buffer = np.zeros((buffer_size,4))\n",
    "\n",
    "buffer_count = 0\n",
    "buffer_full = False\n",
    "for episode in range(num_of_episodes): # go through the episodes\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0 # reward starts as 0 for each episode\n",
    "\n",
    "    if episode % 100 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: \n",
    "        \n",
    "        # Q-net prediction\n",
    "        Q_net_pred = model.predict(state.reshape(1,-1))\n",
    "\n",
    "        # Select random or prediction        \n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(Q_net_pred)\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n) # do a random ation\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward # add the reward\n",
    "        \n",
    "        # Store buffer variables\n",
    "        if buffer_count < buffer_size:\n",
    "            buf_ind = buffer_count\n",
    "        else:\n",
    "            buffer_count = 0\n",
    "            buf_ind = buffer_count\n",
    "            buffer_full = True\n",
    "            \n",
    "        state_buffer[buf_ind,:] = state\n",
    "        action_buffer[buf_ind] = action\n",
    "        reward_buffer[buf_ind] = reward\n",
    "        new_state_buffer[buf_ind,:] = new_state\n",
    "        done_buffer[buf_ind] = done\n",
    "        \n",
    "        # Increments\n",
    "        state = new_state\n",
    "        buffer_count = buffer_count+1\n",
    "\n",
    "    if epsilon > 0.05: # epsilon modification\n",
    "        if episode_reward > prior_reward and episode > 1000:\n",
    "            epsilon = math.pow(epsilon_decay_value, episode - 1000)\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    total_reward += episode_reward # episode total reward\n",
    "    prior_reward = episode_reward\n",
    "\n",
    "    if buffer_full and episode % tr_freq == 0: \n",
    "        # Train network\n",
    "        X = np.zeros((tr_batch_size,4))\n",
    "        Y = np.zeros((tr_batch_size,2))\n",
    "        for ind, tr_ind in enumerate(np.random.randint(buffer_size,size=tr_batch_size)):\n",
    "            X[ind,:] = state_buffer[tr_ind,:]\n",
    "            Y[ind,:] = model.predict(X[ind,:].reshape(1,-1))\n",
    "            if done_buffer[tr_ind]:\n",
    "                Y[ind, int(action_buffer[tr_ind])] = reward\n",
    "            else:                \n",
    "                Y[ind, int(action_buffer[tr_ind])] = reward+gamma*np.max(model.predict(new_state_buffer[tr_ind,:].reshape(1,-1)))\n",
    "        \n",
    "        model.fit(X,Y,epochs=4,verbose=1)\n",
    "        #model.fit(X,Y,verbose=1)\n",
    "        \n",
    "    if episode % 100 == 0: # every 1000 episodes print the average time and the average reward\n",
    "        print(f'Current DQN perf {eval_dqn(model,10,50)}')\n",
    "\n",
    "    if episode % 1000 == 0: # every 1000 episodes print the average time and the average reward\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    " \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a871f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "R.S. Sutton and A.G. Barto (2021): Reinforcement Learning: An Introduction. 2n ed. URL: http://incompleteideas.net/book/the-book.html \n",
    "\n",
    "Christopher Wong: \"FrozenLake\" Blog post. URL: https://cwong8.github.io/projects/FrozenLake/\n",
    "\n",
    "Ali Fakhry (2020): \"Using Q-Learning for OpenAI’s CartPole-v1\" Blog Post. URL: https://medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df\n",
    "\n",
    "Greg Surma: \"Cartpole - Introduction to Reinforcement Learning (DQN - Deep Q-Learning)\" Blog post. URL: https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\n",
    "\n",
    "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller (2013): \"Playing Atari With Deep Reinforcement Learning\", NeurIPS Deep Learning Workshop. URL: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
