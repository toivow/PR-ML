{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural network is largely considered as the state-of-the-art approach to machine learning, but it should be noted that neural networks is not a single method, such as Logistic Regression or SVMs, but comes in various flavors and forms. Here we develop the theory from the simplest form, a single neuron - perceptron, to a complete ML paradigm called \"deep learning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single neuron - \"Perceptron\"\n",
    "\n",
    "Perceptron represents a single neuron and gives its name to neural networks, i.e. a network of multiple interconnected neurons. Its a function that maps inputs, such as $x_1$ and $x_2$ plus bias $-1$, to the desired output $y$, through nonlinear mapping such as logistic sigmoid, i.e. $y=logsig(w_1x_1+w_2x_2-w_0)$. The neuron \"learns\" from training data by adjusting its weights, $\\vec{w}=(w_0, w_1, w_2)^T$, through gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Training a single neuron to represent $x_1$ OR $x_2$ logical function\n",
    "Let's train a single neuron using the gradient rules derived in the lectures. In this example you should try different values for the learning rate and initialization of the network weights. The both affect to speed of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training data is x_1 OR x_2 function\n",
    "x = np.array([[0,0],[1,0],[1,1],[0,1]])\n",
    "y = np.array([[0,1,1,1]]).T\n",
    "\n",
    "# Training parameters\n",
    "w_t = [0,0,0] # w1 w2 w0 - good practice is to initialize random (test: [-10,0,0] [-100,0,0])\n",
    "num_of_epochs = 1000\n",
    "learning_rate = 0.5\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "MSE = np.zeros([num_of_epochs,1])\n",
    "for e in range(num_of_epochs):\n",
    "    for n in range(x.shape[0]):\n",
    "        y_hat = sigmoid(w_t[0]*x[n,0] + w_t[1]*x[n,1] - w_t[2])\n",
    "        sigma_sigmoid = sigmoid(y_hat)*(1-sigmoid(y_hat))\n",
    "        \n",
    "        sigma_w1 = -2*(y[n]-y_hat)*sigma_sigmoid*x[n,0]\n",
    "        sigma_w2 = -2*(y[n]-y_hat)*sigma_sigmoid*x[n,1]\n",
    "        sigma_w0 = 2*(y[n]-y_hat)*sigma_sigmoid\n",
    "\n",
    "        w_t[0] = w_t[0] - learning_rate*sigma_w1\n",
    "        w_t[1] = w_t[1] - learning_rate*sigma_w2\n",
    "        w_t[2] = w_t[2] - learning_rate*sigma_w0\n",
    " \n",
    "    y_h = np.zeros([x.shape[0],1])\n",
    "    for n in range(x.shape[0]):\n",
    "        y_h[n] = sigmoid(w_t[0]*x[n,0] + w_t[1]*x[n,1] - w_t[2])\n",
    "    MSE[e] = np.sum((y-y_h)**2)\n",
    "\n",
    "print(w_t)\n",
    "for n in range(x.shape[0]):\n",
    "    y_h[n] = sigmoid(w_t[0]*x[n,0] + w_t[1]*x[n,1] - w_t[2])\n",
    "    print(f'GT: y({n})={y[n]} ; Pred: y_h({n})={y_h[n]}')\n",
    "plt.plot(range(num_of_epochs),MSE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A full-connected network\n",
    "\n",
    "A full-connected neural network ala \"Multi-layer Perceptron\" is the vanilla ice cream of neural computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a fully connected network of three neurons to represent $x_1$ XOR $x_2$\n",
    "\n",
    "We implement the backward pass of gradient to train this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training data is x_1 XOR x_2 function\n",
    "x = np.array([[0,0],[1,0],[1,1],[0,1]])\n",
    "y = np.array([[0,1,0,1]]).T\n",
    "\n",
    "# Training parameters\n",
    "#w_1_t = [-10,-10,-10] # w1 w2 w0\n",
    "w_1_t = np.random.normal(0,1,[3,1])\n",
    "#w_2_t = [100,-100,50] # w1 w2 w0\n",
    "w_2_t = np.random.normal(0,1,[3,1])\n",
    "#w_3_t = [-100,100,50] # w1 w2 w0\n",
    "w_3_t = np.random.normal(0,1,[3,1])\n",
    "num_of_epochs = 1000\n",
    "learning_rate = 0.5\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def neuron_forward(x1,x2,w1,w2,w0):\n",
    "    return sigmoid(w1*x1+w2*x2-w0)\n",
    "\n",
    "\n",
    "MSE = np.zeros([num_of_epochs,1])\n",
    "for e in range(num_of_epochs):\n",
    "    \n",
    "    # Backward flow and tuning of weights    \n",
    "    y_hat_sum = 0\n",
    "    for n in range(x.shape[0]):\n",
    "        \n",
    "        x1 = x[n,0]\n",
    "        x2 = x[n,1]\n",
    "        y_gt = y[n]\n",
    "        \n",
    "        # Forward flows from inputs to outputs\n",
    "        y_2 = neuron_forward(x1,x2,w_2_t[0],w_2_t[1],w_2_t[2])\n",
    "        y_3 = neuron_forward(x1,x2,w_3_t[0],w_3_t[1],w_3_t[2])\n",
    "        y_1 = neuron_forward(y_2,y_3,w_1_t[0],w_1_t[1],w_1_t[2])\n",
    "\n",
    "        # Backward flow and weight updates\n",
    "        \n",
    "        # Loss gradient\n",
    "        sigma_loss = -2*(y_gt-y_1)\n",
    "\n",
    "        # f1 weight gradients\n",
    "        sigma_f1_w1 = sigma_loss*y_1*(1-y_1)*y_2\n",
    "        sigma_f1_w2 = sigma_loss*y_1*(1-y_1)*y_3\n",
    "        sigma_f1_w0 = -sigma_loss*y_1*(1-y_1)\n",
    "        \n",
    "        # f1 update\n",
    "        w_1_t[0] = w_1_t[0] - learning_rate*sigma_f1_w1\n",
    "        w_1_t[1] = w_1_t[1] - learning_rate*sigma_f1_w2\n",
    "        w_1_t[2] = w_1_t[2] - learning_rate*sigma_f1_w0\n",
    "        \n",
    "        # f1 gradient backward flow\n",
    "        sigma_f1_f2 = sigma_loss*y_1*(1-y_1)*w_1_t[0]\n",
    "        sigma_f1_f3 = sigma_loss*y_1*(1-y_1)*w_1_t[1]\n",
    "\n",
    "        # f2 weight gradients\n",
    "        sigma_f2_w1 = sigma_f1_f2*y_2*(1-y_2)*x1\n",
    "        sigma_f2_w2 = sigma_f1_f2*y_2*(1-y_2)*x2\n",
    "        sigma_f2_w0 = -sigma_f1_f2*y_2*(1-y_2)\n",
    "        \n",
    "        # f2 update\n",
    "        w_2_t[0] = w_2_t[0] - learning_rate*sigma_f2_w1\n",
    "        w_2_t[1] = w_2_t[1] - learning_rate*sigma_f2_w2\n",
    "        w_2_t[2] = w_2_t[2] - learning_rate*sigma_f2_w0\n",
    "\n",
    "        # f3 weight gradients\n",
    "        sigma_f3_w1 = sigma_f1_f3*y_3*(1-y_3)*x1\n",
    "        sigma_f3_w2 = sigma_f1_f3*y_3*(1-y_3)*x2\n",
    "        sigma_f3_w0 = -sigma_f1_f3*y_3*(1-y_3)\n",
    "        \n",
    "        # f3 update\n",
    "        w_3_t[0] = w_3_t[0] - learning_rate*sigma_f3_w1\n",
    "        w_3_t[1] = w_3_t[1] - learning_rate*sigma_f3_w2\n",
    "        w_3_t[2] = w_3_t[2] - learning_rate*sigma_f3_w0\n",
    "\n",
    " \n",
    "    y_h = np.zeros([x.shape[0],1])\n",
    "    for n in range(x.shape[0]):\n",
    "        y_2 = neuron_forward(x[n,0],x[n,1],w_2_t[0],w_2_t[1],w_2_t[2])\n",
    "        y_3 = neuron_forward(x[n,0],x[n,1],w_3_t[0],w_3_t[1],w_3_t[2])\n",
    "        y_1 = neuron_forward(y_2,y_3,w_1_t[0],w_1_t[1],w_1_t[2])\n",
    "        y_h[n] = y_1\n",
    "    MSE[e] = np.sum((y-y_h)**2)\n",
    "\n",
    "print(w_2_t)\n",
    "print(w_3_t)\n",
    "print(w_1_t)\n",
    "for n in range(x.shape[0]):\n",
    "    y_2 = neuron_forward(x[n,0],x[n,1],w_2_t[0],w_2_t[1],w_2_t[2])\n",
    "    y_3 = neuron_forward(x[n,0],x[n,1],w_3_t[0],w_3_t[1],w_3_t[2])\n",
    "    y_1 = neuron_forward(y_2,y_3,w_1_t[0],w_1_t[1],w_1_t[2])\n",
    "    y_h[n] = y_1\n",
    "    print(f'Input x=({x[n,:]}) GT: y({n})={y[n,0]:.2f} ; Pred: y_h({n})={y_h[n,0]:.2f}')\n",
    "plt.plot(range(num_of_epochs),MSE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network\n",
    "\n",
    "For large images the full-connected network has too many parameters to train and it does not learn small translation invariance which is essential, but all these problems are solved by convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla neural network using Keras and TensorFlow for MNIST Handwritten Digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(f' Min y (class) values is {np.min(y_test)} and max {np.max(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then display examples of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(x_train.shape)\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4,figsize=[12,3])\n",
    "ax1.imshow(x_train[0,:,:])\n",
    "ax1.set_title(f'Class number {y_train[0]}')\n",
    "ax2.imshow(x_train[1,:,:])\n",
    "ax2.set_title(y_train[1])\n",
    "ax3.imshow(x_train[2,:,:])\n",
    "ax3.set_title(y_train[2])\n",
    "ax4.imshow(x_train[3,:,:])\n",
    "ax4.set_title(y_train[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make one full connected layer. You may play with the number of neurons in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Flatten input image to a vector\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "print(model.output_shape)\n",
    "\n",
    "# Add a full connected layer\n",
    "model.add(keras.layers.Dense(32, activation='sigmoid'))\n",
    "print(model.output_shape)\n",
    "\n",
    "# Add final layer for 10 classes (one-hot encoding)\n",
    "model.add(keras.layers.Dense(10,activation='sigmoid'))\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loss takes care of one-hot encoding (see https://keras.io/api/losses/)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#loss_fn = tf.keras.losses.MeanSquaredError(from_logits=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well it does without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test before training (rand accuracy is 1/10)\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train for some number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the trained model with our witheld test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model.predict(x_test)\n",
    "y_test_hat = y_test_hat[0:10,:]\n",
    "#print(np.maxind(y_test_hat))\n",
    "print(y_test[0:10])\n",
    "\n",
    "print(np.argmax(y_test_hat,axis=1))\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential()\n",
    "\n",
    "model2.add(keras.layers.Input(shape=(28,28,1)))\n",
    "print(model2.output_shape)\n",
    "           \n",
    "# Flatten input image to a vector\n",
    "model2.add(keras.layers.Conv2D(16,kernel_size=(5,5),strides=(2,2)))\n",
    "print(model2.output_shape)\n",
    "\n",
    "# Flatten input image to a vector\n",
    "model2.add(keras.layers.Flatten())\n",
    "print(model2.output_shape)\n",
    "\n",
    "# Add a full connected layer\n",
    "model2.add(keras.layers.Dense(10, activation='sigmoid'))\n",
    "print(model2.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This loss takes care of one-hot encoding (see https://keras.io/api/losses/)\n",
    "#loss_fn2 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "##loss_fn = tf.keras.losses.MeanSquaredError(from_logits=True)\n",
    "#model2.compile(optimizer='adam',\n",
    "#              loss=loss_fn2,\n",
    "#              metrics=['accuracy'])\n",
    "#model2.summary()\n",
    "\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert training data to format assumed by a convolutional filter (add one more dimension to make it explicit) and convert y explicitly to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(x_train, y_train_cat, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model2.predict(x_test)\n",
    "y_test_hat = y_test_hat[0:10,:]\n",
    "#print(np.maxind(y_test_hat))\n",
    "print(y_test[0:10])\n",
    "\n",
    "print(np.argmax(y_test_hat,axis=1))\n",
    "model2.evaluate(x_test,  y_test_cat, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is with more tricks and flavors but we need to learn about backbone networks first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With more tricks (ReLu and MaxPooling)\n",
    "model2 = tf.keras.models.Sequential()\n",
    "\n",
    "model2.add(keras.layers.Input(shape=(28,28,1)))\n",
    "print(model2.output_shape)\n",
    "           \n",
    "# Flatten input image to a vector\n",
    "model2.add(keras.layers.Conv2D(16,kernel_size=(3,3)))\n",
    "print(model2.output_shape)\n",
    "\n",
    "# Flatten input image to a vector\n",
    "model2.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "print(model2.output_shape)\n",
    "\n",
    "# Flatten input image to a vector\n",
    "model2.add(keras.layers.Conv2D(32,kernel_size=(3,3)))\n",
    "print(model2.output_shape)\n",
    "\n",
    "# Flatten input image to a vector\n",
    "model2.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "print(model2.output_shape)\n",
    "\n",
    "\n",
    "# Flatten input image to a vector\n",
    "model2.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "print(model2.output_shape)\n",
    "\n",
    "# Add dropout \"layer\"\n",
    "model2.add(keras.layers.Dropout(0.2))\n",
    "print(model2.output_shape)\n",
    "\n",
    "# Add a full connected layer\n",
    "model2.add(keras.layers.Dense(10, activation='softmax'))\n",
    "print(model2.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "TensorFlow Tutorials. URL: https://www.tensorflow.org/tutorials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
